{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import plotnine\n",
    "from plotnine import ggplot, aes, facet_wrap, scale_x_log10\n",
    "plotnine.theme_set(plotnine.theme_bw())\n",
    "\n",
    "from load_hess import hessTrain, hessTrainY, hessTest, hessTestY, probeAnnot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often assumed that in any one particular problem the expression\n",
    "patterns of most genes---or, more generally, the values of most\n",
    "features in a high-dimensional data set---are either:\n",
    "-   uninformative or\n",
    "-   redundant with a few maximally useful markers.\n",
    "\n",
    "*Feature selection* attempts to identify a restricted set of such\n",
    "useful features for inclusion in a classification model while\n",
    "discarding the rest.\n",
    "\n",
    "While feature selection may or may not improve the performance of a\n",
    "particular classifier applied to a particular problem, it can\n",
    "1.  reduce computational workload,\n",
    "2.  help to avoid overfitting\n",
    "    -   (though feature selection can itself be susceptible to\n",
    "        overfitting!), and\n",
    "    \n",
    "3.  facilitate assessment of model using less high-throughput\n",
    "    platforms.\n",
    "\n",
    "A good, if somewhat dated, reference on the use of feature selection\n",
    "methods in bioinformatics is [@saeys2007review], which breaks down\n",
    "feature selection methods into the following three categories:\n",
    "\n",
    "**Filter**\n",
    ":   Selection done before and independently of\n",
    "    classifier construction. Can be univariate or multivariate.\n",
    "\n",
    "**Wrapper**\n",
    ":   Embed classifier construction within feature\n",
    "    selection process. Heuristic search methods compare models, favor\n",
    "    adding or removing features based on optimization of some\n",
    "    specified metric on resulting classifiers.\n",
    "\n",
    "**Embedded**\n",
    ":   Feature selection is inherently built into\n",
    "    some classifier construction methods.\n",
    "\n",
    "For a nice figure providing more detail on the advantages and\n",
    "disadvantages associated with each of these categories, along with\n",
    "some standard examples, you can follow the link to\n",
    "<https://academic.oup.com/view-large/1739292>.\n",
    "\n",
    "I can't do justice to the wide range of feature selection techniques\n",
    "in the limited time available in this course, so we're going to focus\n",
    "on one particularly simple method: a plain old $t$-test.\n",
    "`sklearn.feature_selection` (`fs` here) has this built-in:\n",
    "`fs.f_regression`\n",
    "\n",
    "In order to use this feature selection method as part of a\n",
    "classification \"pipeline\", we need to connect it (\"upstream\") to a\n",
    "(\"downstream\") classification algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs\n",
    "import sklearn.neighbors as nbr\n",
    "import sklearn.pipeline as pl\n",
    "fsKnnPipeline = pl.Pipeline([\n",
    "    (\"featsel\", fs.SelectKBest(fs.f_regression, k=25)),\n",
    "    (\"classifier\", nbr.KNeighborsClassifier(n_neighbors=9))\n",
    "])\n",
    "fsKnnFit = deepcopy(fsKnnPipeline).fit(hessTrain.T, hessTrainY)\n",
    "fsKnnTestPredictionProbs = fsKnnFit.predict_proba(hessTest.T)\n",
    "fsKnnTestPredictionProbs[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsKnnTestPredictionClass = fsKnnFit.predict(hessTest.T)\n",
    "fsKnnTestPredictionClass[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(fsKnnTestPredictionClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-use the learner pipeline `fsKnnPipeline` for\n",
    "cross-validation using `cross_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "np.random.seed(321)\n",
    "shuffle = np.random.permutation(len(hessTrainY))\n",
    "trainShuffled = hessTrain.T.iloc[shuffle]\n",
    "trainYShuffled = hessTrainY[shuffle]\n",
    "## - set up cvScheduler to generate cross-validation folds\n",
    "cvScheduler = ms.KFold(n_splits=5)\n",
    "## can re-use same fsKnnPipeline object implementing ML aglorithm from above\n",
    "fsKnnCvAccs = ms.cross_val_score(estimator = fsKnnPipeline,\n",
    "                                 X = trainShuffled,\n",
    "                                 y = trainYShuffled,\n",
    "                                 cv = cvScheduler.split(trainShuffled))\n",
    "np.mean(fsKnnCvAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` also supports selection of optimal parameter settings via\n",
    "parameter grid search using `sklearn.model_selection.GridSearchCV`;\n",
    "you don't need to create the whole grid yourself with `sklearn`, just\n",
    "pass in the range of values you want for each parameter. For `Pipeline`\n",
    "fits, the full parameter names are specified in\n",
    "<component-name>__<within-component-feature-name> format, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Cross-validated parameter grid search using sklearn:\n",
    "gscv = ms.GridSearchCV(fsKnnPipeline,\n",
    "                       {\"featsel__k\" : [10, 100, 1000, 10000],\n",
    "                        \"classifier__n_neighbors\" : [5, 9, 19]},\n",
    "                       cv = cvScheduler.split(trainShuffled))\n",
    "gscv.fit(trainShuffled, trainYShuffled)\n",
    "gscv.best_estimator_.named_steps[\"featsel\"].k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_estimator_.named_steps[\"classifier\"].n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that for this data set the size of the\n",
    "selected feature set will vary considerably upon re-running the\n",
    "cross-validation---but only if you change the seed passed into\n",
    "`np.random.seed` before generating `shuffle`!\n",
    "\n",
    "Feature Extraction \n",
    "==================\n",
    "\n",
    "An alternative approach to feature selection to mitigating the\n",
    "problems of overfitting and high computational workload associated\n",
    "with machine learning with high-dimensional data is *feature extraction*.\n",
    "\n",
    "While\n",
    "\n",
    "**feature selection**\n",
    ":   reduces the size of the feature set\n",
    "    presented to a classification or regression algorithm by retaining\n",
    "    only a small subset of the feature set,\n",
    "\n",
    "**feature extraction**\n",
    ":   applies a mathematical\n",
    "    transformation to the high-dimensional input data to derive a\n",
    "    low-dimensional feature set.\n",
    "\n",
    "For example, if you were trying to classify day vs. night situations\n",
    "with digital image data, you could simply average the intensities of\n",
    "all pixels together to extract a \"light level\" feature. Note that\n",
    "this single extracted feature still depends on the value of *all*\n",
    "of the input features, so it doesn't reduce the amount of data you\n",
    "need to collect to evaluate the model, but it does massively diminish\n",
    "the complexity of the task confronting whatever downstream\n",
    "classification algorithm you apply!\n",
    "\n",
    "With gene expression data, the most obvious and widely used method of\n",
    "feature extraction is PCA, so we will use this for our example. Recall\n",
    "that the PC1 scores of a sample are defined as a weighted sum (or\n",
    "linear combination) of feature values with the feature weights learned\n",
    "so as to optimally model feature values based on (feature mean +\n",
    "feature weight * sample score). Higher PCs can then be defined so as\n",
    "to in a similar way so as to successively improve the model.\n",
    "\n",
    "When building a classification or regression model using PCA for\n",
    "feature extraction, we learn the feature weights for the various\n",
    "principal components (which make up the elements of the \"rotation\n",
    "matrix\"), as well as the feature mean values, using the training set\n",
    "(only). These weights and means are then fixed parameters of the fit\n",
    "model and should not be updated when presented with test data!\n",
    "\n",
    "Here is a function for learning the PCs from a training set (provided\n",
    "to the function as a matrix of feature values `mat`) which returns a\n",
    "function `extractor` for assessing the sample scores for a test set\n",
    "`newdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## arguments to extractPCs\n",
    " ## - mat is matrix or data.frame of feature values\n",
    " ## - m is number of principal component features to extract\n",
    "def extractPCs(mat, m=None, *args):\n",
    "     ## assume x is samples-in-rows, genes-in-columns format!\n",
    "    if m is None:\n",
    "        m = np.min(mat.shape)    \n",
    "    mu = mat.mean(axis=0)  ## training-set estimated mean expression per gene\n",
    "     ## use singular value decomposition (SVD) to compute PCs:\n",
    "    svdOut = np.linalg.svd(mat - mu, full_matrices=False)\n",
    "    x = svdOut[0] * svdOut[1]  ## same as R's prcomp out$x\n",
    "    rotation = svdOut[2].T     ## same as R's prcomp out$rotation\n",
    "    sdev = svdOut[1] / np.sqrt(len(svdOut[1])-1)  ## same as R's prcomp out$sdev\n",
    "    extractor = lambda newdata : np.dot(newdata-mu, rotation[:, 0:m])\n",
    "    extractor.sdev = sdev\n",
    "    extractor.rotation = rotation    \n",
    "    extractor.center = mu\n",
    "    extractor.x = x\n",
    "    extractor.m = m\n",
    "     ## return the function \"extractor\" which can be applied to newdata;\n",
    "     ## this function yields coordinates of samples in newdata in PC-space\n",
    "     ## learned from the training data passed in as x argument.\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to wrap `extractPCs` function up into `sklearn`-compatible\n",
    "class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcaExtractor(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"Transforms data set into first m principal components\"\n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "    def fit(self, X, y=None):\n",
    "        self.extractor = extractPCs(X, m=self.m)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return self.extractor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hook a `PcaExtractor` object for learning the PC features to\n",
    "extract from data up to our knn classification algorithm in a manner\n",
    "similar to what we did for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaKnnPipeline = pl.Pipeline([\n",
    "    (\"featextr\", PcaExtractor(m=5)),\n",
    "    (\"classifier\", nbr.KNeighborsClassifier(n_neighbors=9))\n",
    "])\n",
    "pcaKnnFit = deepcopy(pcaKnnPipeline).fit(hessTrain.T, hessTrainY)\n",
    "pcaKnnTestPredictionClass = pcaKnnFit.predict(hessTest.T)\n",
    "pd.crosstab(pcaKnnTestPredictionClass, hessTestY,\n",
    "            rownames=[\"prediction\"], colnames=[\"actual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again we can re-use the `pcaKnnPipeline` learner object\n",
    "to do cross-validation on the training set (this is a bit slow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaKnnCvAccs = ms.cross_val_score(estimator = pcaKnnPipeline,\n",
    "                                  X = trainShuffled,\n",
    "                                  y = trainYShuffled,\n",
    "                                  cv = cvScheduler.split(trainShuffled))\n",
    "np.mean(pcaKnnCvAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for a parameter grid search (this may take a while!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Cross-validated parameter grid search using sklearn:\n",
    " ## (may take a while!)\n",
    "gscv = ms.GridSearchCV(pcaKnnPipeline,\n",
    "                       {\"featextr__m\" : [3, 4, 5],\n",
    "                        \"classifier__n_neighbors\" : [5, 9, 19]},\n",
    "                       cv = cvScheduler.split(trainShuffled))\n",
    "gscv.fit(trainShuffled, trainYShuffled)\n",
    "gscv.best_estimator_.named_steps[\"featextr\"].m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_estimator_.named_steps[\"classifier\"].n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going deeper \n",
    "============\n",
    "\n",
    "It may have occurred to you that feature selection can be seen as a\n",
    "particularly simple type of feature extraction which \"transforms\"\n",
    "the input feature matrix by simply projecting it onto a few select\n",
    "dimensions.\n",
    "\n",
    "Similarly, just as we previously described the transformation of a\n",
    "data set by PCA feature extraction as a type of prediction, we could\n",
    "reverse viewpoints and frame the action of `predict` methods as\n",
    "really just another type of data transformation---albeit with some\n",
    "peculiar restrictions on the transformed output values (e.g., must be\n",
    "probability scores, must be class labels from particular set of\n",
    "possibilities, etc.).\n",
    "\n",
    "From this point of view, an ML pipeline is an ordered sequence of ML\n",
    "algorithm steps. To train such a pipeline, we go through the sequence:\n",
    "-   training the $i^{\\text{th}}$ ML algorithm step using the\n",
    "    transformed output from step $i-1$ as our input feature matrix for\n",
    "    the current step,\n",
    "-   thus learning fit submodel $i$.\n",
    "-   We then transform the output again using our newly trained fit\n",
    "    submodel $i$ and pass it along as input feature matrix to ML\n",
    "    algorithm $i+1$.\n",
    "\n",
    "To make predictions using the fit pipeline model resulting from this\n",
    "training procedure, we iterate through the the ordered sequence of\n",
    "trained submodels, taking the transformed output from step $i-1$ as\n",
    "input feature matrix to be transformed by fit submodel $i$ and then\n",
    "passed along to step $i+1$. The predicted values are then whatever is\n",
    "output from the final step of the fit pipeline.\n",
    "\n",
    "The field of *deep learning*\n",
    "([@goodfellow2016deep]) builds such pipelines out of individual\n",
    "steps (\"layers\") for which the argument feature matrix and output\n",
    "transformed feature matrix are similar enough in nature that the same\n",
    "type of submodel can be linked together repeatedly to generate very\n",
    "long pipelines. Because each individual layer in a deep learning model\n",
    "is itself generally composed of many similar subunits (artificial\n",
    "\"neurons\"), the structure of a deep learning model is typically\n",
    "referred to as a *network* instead of a pipeline, and we speak of\n",
    "a many-layer network as being *deep* instead of long.\n",
    "\n",
    "Deep learning is beyond the scope of this course, but if you work on\n",
    "any projects involving machine learning long enough it's bound to come\n",
    "up at some point. Especially in problems with very large numbers of\n",
    "$n$ sampling units, deep learning models can often outperform other\n",
    "methods, though they are prone to overfitting and tend to require a\n",
    "great deal of time and effort to get working correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
